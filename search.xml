<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>About Me</title>
    <url>/1997/06/08/AboutMe/</url>
    <content><![CDATA[<p>I’m Zhilin Huang, a Master student from Peking Univerity. I received the B.S. degree from Harbin Institute of Technology, Harbin, China. My research interests include image inpainting, multimodal learning and semantic segmentation.</p>
<p>Contact me: <a href="mailto:&#122;&#x65;&#114;&#105;&#110;&#104;&#x77;&#97;&#110;&#103;&#48;&#x33;&#64;&#x70;&#107;&#117;&#46;&#x65;&#100;&#x75;&#x2e;&#99;&#110;">&#122;&#x65;&#114;&#105;&#110;&#104;&#x77;&#97;&#110;&#103;&#48;&#x33;&#64;&#x70;&#107;&#117;&#46;&#x65;&#100;&#x75;&#x2e;&#99;&#110;</a></p>
]]></content>
  </entry>
  <entry>
    <title>CISI-net:Explicit Latent Content Inference and Imitated Style Rendering for Image Inpainting</title>
    <url>/2020/10/24/CISI/</url>
    <content><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>基于neural patch（contextual attention等attention方法）的方法不太合理之处在于：might introduce undesired content change in the predicted region, especially when the desired content does not exist in the known region.</p>
<p>这篇文章就是为了解决上述的问题，通过显式地将image data分解为content和style code。提出了<strong>content inference and style imitation network (CISI-net)</strong></p>
<a id="more"></a>

<p>其中content inference部分，by performing inference in the latent space to infer the content code of the corrupted images similar to the one from the original images.通过在latent space中去infer出hole内和original images中相接近的content code。这里的能够更有效地预测出content code是因为相比于直接去预测整个image的content而言，前者处理的数据的分布的维度更低。可以理解为前者只需要infer出一个vector，而后者需要从image的pixel-level来进行inference，也就是HxWx3的维度。</p>
<p>对于style部分，被用于表示整个图像的风格。和整个image保持一致。</p>
<blockquote>
<p>所以对于image inpainting任务而言，将待处理的image data分解为content和style是完全合理的。但是还有一个问题，就在于这部分信息应该如何被利用。可以参考一下风格迁移的相关方法，有没有从content和style的角度分别利用normalization的方式来实现这种generation任务的。因为normalization有一个优势就是可以实现information propagation。</p>
</blockquote>
<p>除此之外：However, the matching process is quite time consuming. Moreover, since the neural patch is a mixture of content and style, copying them from known region into missing region in the second stage will introduce change to the originally generated content, still leading to some notable artifacts。首先matching process十分消耗时间；其次，直接操作每个level的neural patch，忽略了每个patch中实际上同时包含有content和style的信息，直接这么操作反而会引入模糊，导致artifacts。</p>
<p>这篇paper从人类修复图像的角度出发：（1）首先要去了解图片所处的场景，即prior；（2）其次，恢复出structure；（3）最后根据整个图片的风格，对恢复出来的粗略的structure来进行上色。</p>
<p>所以整个inpainting任务就被分解成了两个子任务：（1）如何合理地从known regions中预测出holes内的semantic content；（2）如何模仿known regions来对hole内预测出的structure进行上色；</p>
<h3 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h3><p><strong>Image Inpainting</strong></p>
<p>已有一些two-stage的方法，在第一阶段尝试去恢复出content信息，而在第二阶段强调texture details的refine。</p>
<blockquote>
<p>这些方法是有借鉴意义的，在于关注要从哪个角度去捕获structure和texture信息。也就是说如何对这两部分信息进行结耦。除此之外。可以参考风格迁移的相关文章。</p>
</blockquote>
<p>或者是从coarse-to-fine的角度，先恢复出coarse results，之后在此基础上恢复出refine的结果</p>
<p>但是这些方式也有问题：However, the matching process is quite time consuming. Moreover, since the neural patch is a mixture of content and style, copying them from known region into missing region in the second stage will introduce change to the originally generated content, still leading to some notable artifacts.</p>
<p>首先matching process十分消耗时间；其次，直接操作每个level的neural patch，忽略了每个patch中实际上同时包含有content和style的信息，直接这么操作反而会引入模糊，导致artifacts。</p>
<p><strong>Style transfer</strong></p>
<p>这篇paper中refine部分的出发点在于style transfer。其中content和style都是从known regions中预测出来的。</p>
<blockquote>
<p>所以这部分的inference/reasoning的工作，完全可以尝试通过graph的方式来得到。通过对known regions中的信息进行编码，再利用graph来对hole内的信息进行infer/reason</p>
</blockquote>
<p>为了避免distortion，相关工作在content image和style image之间进行neural-patch based similarity matching。然而这样的过程要求大量的pixel-level的optimization。</p>
<p><strong>Learning Disentangled Representation</strong></p>
<blockquote>
<p>这部分内容很有参考价值。包括在后续text-guided image inpainting中也有参考价值。</p>
</blockquote>
<p>More recent work focuses on learning hierarchical feature representations using deep convolutional neural networks to separate content and style。近期的工作集中于通过deep CNN学习hierarchical feature representations来分离content和style。</p>
<p>其实关于style和content的定义，在这类任务中根据不同的任务都有着独立的定义。这篇文章中，<strong>将content定义为the underlining spatial structure，而将style定义为the rendering of the structure。实际上就是structure and texture</strong></p>
<p>In this work, we attempt to separate the known region into content and style, and use style to render the predicted content for the missing region.</p>
<h3 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h3><p>这篇paper中，通过两个encoder，来分别对content和style进行编码。并且将它们融合到decoder中。其中，在encoder过程中，主要关注的是对structure的inference；而在decoder中，则重点关注对于known regions的模仿并对预测出来的content进行上色。</p>
<p>相比于之前的two-stage结构，这里将content和style combine到一个stage中。</p>
<p>对于content inference process，基于以下两个assumption：（1）corrupted区域内的the inferred content codes应该和knwon regions中的codes相一致；（2）对于具有不同masks的同一张图片，infer出来的content code需要尽可能保持一致。基于以上的假设，这里对于latent code加入content loss。通过将gt image作为引导，使得所有加入不同masks的相同image预测得到的content code能够尽可能地保持一致。</p>
<p>而对于style code，被视为是整个image的global feature，从而保持hole regions和known regions中的style能够保持一致。这里对于style code的处理收启发与AdaIN（应用于style transfer）。在decode之前，将style code和content code进行融合。</p>
<blockquote>
<p>感觉这里对于style和content code的处理和之前想的非常像了。只不过之前是希望使用conditional batch norm来进行处理，而这里是使用AdaIN来完成。</p>
</blockquote>
<p>main contributions：</p>
<p>（1）将highh-dim features的预测转换为两个low-dim code的预测；</p>
<p>（2）换了一种inpainting的思路，将任务拆分成content和style的预测，然后融合到一个single-stage的网络中进行修复；</p>
<p><strong>Keywords</strong>：Style Transfer，Learning disentangled representation</p>
<p>对于texture details的恢复，有采用residual learning的方式。也有通过information propagation的方式（包括CA，shiftNet）来实现。</p>
<p>这篇paper尝试通过decoder中融合style code的方式来恢复出texture details。即将overall texture details进行编码得到style code。</p>
<p>这篇paper中，采用的类似是AdaIN的实现方式：通过VGG提取出style和content features，并且通过normalization的方式将content融合进styles中。而这里则是通过从corrupted images中提取出content code，在uncorrupted images中提取出style code。</p>
<hr>
<p>作者假定，一个image可以由一个latent content code和latent style code生成。其中，encoder分别对image进行编码，得到latent content code和style code。这里对于encoder加入的约束是：对于content code，希望从gt image中提取出来的content code和从corrupted image中提取出来的content code能够保持一致；而对于style code也同样如此，因为这里希望style code是针对overall的style</p>
<blockquote>
<p>这里实际上可以加入semantic的思想，即每个semantics都有对应的style和content</p>
</blockquote>
<p><strong>这里的style code设计为overall的style就是为了能够有效地在entire image中保持一致性。所以如果要从semantics的角度出发是可以拓展的。</strong></p>
<p>同时，由于style是关于overall的描述，所以这里希望从corrupted image得到的style code能够和gt image得到的style code保持一致。</p>
<blockquote>
<p>但是如果要从semantic的角度来做，也没有问题，反正有gt image，依旧可以做，例如从gt image中生成semantic masks来作为gt semantic masks。这也是一种约束。之后再根据每个semantics来进行对于的style。</p>
<p>感觉也可以参考SEAN</p>
</blockquote>
<p><strong>Overview</strong></p>
<p>（1）Self-consistency：Image可以分解为style code和content code，同时也可以由这两个latent code生成对应的image；</p>
<p>（2）Inferring-consistency：从corrputed image中提取出的style code和content code应该尽可能和从original image中提取出的style code和content code保持一致；</p>
<p>（3）Mutual-consistency：对于从corrupted image中提取出来的style和content code应该与corrupted 的区域无关，仅于original image有关。</p>
<blockquote>
<p>对于这一点有疑问：</p>
<p>包括后续要针对这篇paper进行改进，实际上也可以从这几个assumption出发来进行改进。</p>
</blockquote>
<p><strong>Note that the current generation loss treats each pixel of the output image equally, which is not desired. It leads a large portion of the loss will be from the known region and make the model pay more attention to the generation of this region rather than the hole. On the other hand, due to the known region as input image, the quality of reconstructed content in this region is Inevitably better than that in the holes, which need to be inferred from this available information.</strong></p>
<blockquote>
<p>这也是一个值得注意的地方：对于image inpainting，hole内生成的结果远比known region中的重要。并且，known regions中生成的结果必然比hole内生成的结果更加理想。因此如果同等对待两部分的GAN Loss显然是不公平的。</p>
</blockquote>
<p><strong>Therefore, this is inconsistence between the distributions of reconstructed content in the two regions and distribution of the known region is naturally closer to that of real image, which makes the local patch discriminator cannot distinguish between the output images and real images.</strong> </p>
<p>其实同样的问题出现在patchGAN中，所以分配比重是很有必要的。</p>
<p>To address this issue, we propose a weighted reconstruction loss and multi-scale patch adversarial loss to improve generated quality in the missing region.</p>
<p>通过multi-scale patchGAN，来更好地关注不同scale下的local和global patches。</p>
<p><strong>For differentiating the hole patches and the known patches, we propose to compute the PatchGAN loss only on the regions, which overlap with the holes.</strong></p>
<p><strong>Framework</strong></p>
<p>两个encoder和一个joint decoder。以及一个discriminator。其中一个encoder用于编码data中的spatial structural information，通过一个high-dim spatial <strong>map</strong>来编码；而另一个encoder用于编码style information，具有global和relatively simple effect。因此这里使用的是low-dim <strong>vector</strong>。</p>
<blockquote>
<p><strong>注意一个是spatial map，一个是global vetcor。所以如果要从这个角度来改进：（1）spatial map对应content的encoder，可以考虑ACF/OCR/GraphModule等模块来重建hole内content；（2）对于style，可以尝试针对每个semantics得到一个style。</strong></p>
</blockquote>
<p><strong>Self-content encoder (SCE).</strong> 用于从complete image中提取content feature。</p>
<p><strong>Inferred-content encoder (ICE).</strong>  相比于SCE，ICE不仅仅需要提取对应的content feature，还需要对corrupted区域的content feature进行预测。**和SR等任务不同之处在于，content inference usually not only rely on local statistics, but also on global context.**也就是说，这部分实际上很需要借助distant contextual information。 </p>
<p><strong>Style encoder (SE).</strong> 由于style feature是一个global effect，关于如何对inferred的content进行renderring。因此他应该能够从同一张image中无论被什么样的masks进行污染中得到相同的features，也就是对于masks不敏感。</p>
<p><strong>Decoder.</strong> 受启发于style transform利用affine transformation parameters于norm layers中来表示style，这里采用Adative Instance Norm layer于residual blocks来调整generated images中的style。</p>
<p><strong>Discriminator.</strong> 这里采用的LSGAN以及multi-scale discriminator来引导生成realistic details和correct global structure。</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Image Prior</title>
    <url>/2020/09/10/DeepImagePrior/</url>
    <content><![CDATA[<h3 id="Target-Motivation"><a href="#Target-Motivation" class="headerlink" title="Target/Motivation"></a>Target/Motivation</h3><p>Deep neural network能够很好地完成image generation和restoration任务，很大程度归功于network从大量的example iamges中学习realistic image priors。在这篇paper中，作者展示了一个事实，就是generator network的结构以及足够为any learning捕获low-level image statistics prior。为了证明这一点，作者通过randomly-initialized neural network也能够作为handcrafted prior在standard inverse problems（例如denosing，inpainting）中也能够有很好的效果。</p>
<a id="more"></a>

<p>Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.</p>
<blockquote>
<p><strong>inverse problem</strong></p>
</blockquote>
<p>The authors show that, in fact, not all image priors must be learned from data; instead, a great deal of image statistics are captured by the structure of generator ConvNets, independent of learning. This is especially true for the statistics required to solve certain image restoration problems, where the image prior must supplement the information lost in the degradation processes.并非所有的image prior都需要从data中学习得到，通过generator的convolutional network structure能够捕获大量的image statistics，而与learning无关。其中这对于image restoration problems很重要，这里的image prior需要能够补充在degradation processes中丢失的information。</p>
<p>Instead of following the standard paradigm of training a ConvNet on a large dataset of example images, we fit a generator network to a single degraded image. In this scheme, the network weights serve as a parametrization of the restored image.也就是说，现在这种paradigm是将一个network适配一个single degraded image。通过这个network作为prior的引导下，以iterative optimization的方式完成image的reconstruction。因此，所有用于完成image reconstruction的信息都存在于（1）single degraded image中；（2）handcrafted network的structure中。</p>
<p>This is particularly remarkable because no aspect of the network is learned from data and illustrates the power of the image prior implicitly captured by the network structure.network没有从data中学习到什么，同时也说明了由network隐式地捕获的image prior的能力。</p>
<p>In this paper, the authors show that an untrained deep convolutional generator can be used to replace the surrogate natural prior.</p>
<h3 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h3><p>作者提出了一个很神奇的发现：<strong>One might think that knowledge about the distribution $p(x)$ is encoded in the parameters $\theta$ of the network, and is therefore learned from data by training the model. Instead, we show here that a significant amount of information about the image distribution is contained in the structure of the network even without performing any training of the model parameters.</strong></p>
<p>也就是说，实际上关于image的distrubution的knowledge并不是在学习的过程中被encoded到network的parameters中，相反地，根据实验结果发现，关于image distribution的一系列information实际上是被包含在network的structure中的，即使这个网络中的参数没有经过任何的训练。</p>
<p>作者通过将neural network视为image的parametrization。从这个角度来看，latent code是一个fixed random tensor，而network则将network的parameters $\theta$（由weights和bias组成）映射到image中。相当于是说，我固定了input的latent code，想要看看network中的parameters $\theta$ 和image的关系。通过这种方式，来看network的参数对应的是什么信息。</p>
<p>由于没有通过数据集进行训练，因此不能够期望network能够知道specific concepts，例如certain objects classes的appearance。然而，通过实验发现，untrained network确实capture了一些natural images的low-level statistics。特别地，卷积的local和translation invariant性质以及一系列此类算子的应用在多个尺度上capture the relationship of pixel neighbourhood.这些性质已经足够使model能够针对image restoration中的conditional image distribution进行建模，可以用来解决inverse problem。</p>
<p>这篇paper并没有去显式地处理distribution，而是formulate这类tasks作为一个energy minimization problems。</p>
<p>$x^*=argmax_x E(x;x_0)+ R(x)$</p>
<p>其中$E(·)$是task-dependent data term，$R(·)$是regularizer，$x_0$是noisy/low-resolution/corrupted image。</p>
<p>$E(·)$通常是由application直接决定的，并不困难。而对于regularizer $R(·)$而言，通常不与specific application相关，因为他capture the generic regularity of natural images.即捕获了natural image的一般规律。最简单的一个日子就是Total Variation，鼓励image包含uniform regions。这篇paper中，不显式地使用regularizer，而是通过neural network parametrization隐式地捕获prior（regularizier本身就是为了捕获natural image的一般规律性质的，因此这部分信息实际上就是image的prior）。</p>
<p>$\theta^*=argmin_{\theta}E(f_{\theta}(z);x_0)$</p>
<p>这里$\theta$的获取就是通过gradient descent的方式得到的。因此可以用于restoration process的empirical information时noisy image。</p>
<p>由于没有从data中学习network的相关信息，因此这种deep image prior是一种有效的handcrafted信息，就像是TV norm。</p>
<p>这篇paper的贡献就在于展示了这种handcrafted prior对于各种image restoration任务而言都有着非常好的效果，远超standard handcrafted priors，甚至能够在一些cases下逼近learning-based approaches。</p>
<p>如实验所示，对于architecture的选择会对结果有着很大的影响。同时input random noise是可以进行优化的，但是这篇paper中没有进行相关的工作，而是将z设为一个fixed randomly-initialized tensor。</p>
<p><strong>A parametrization with high noise impedance</strong></p>
<p>关于为什么high-capacity network可以作为prior，实际上人们希望可以找到parameters来恢复出任何可能的image，包括random noise，因此network不应该对generated image上加入强制的限制。实验结果表明，尽管实际上任何image都可以被model拟合，但是network architecture的选择对于使用gradient descent等方法来找到solution space来说有着很重大的影响。同时实验结果表明，network可以抵制bad solutions，并且可以更快地下降到naturally-looking images上。</p>
<p><strong>“Sampling” from the deep image prior</strong></p>
<p>Adding skip connections results in images that contain structures of different characteristic scales, as is desir- able for modeling natural images.UNet的架构表现是最好的。</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Attentive Normalization for Conditional Image Generation</title>
    <url>/2020/08/20/AttentiveNorm/</url>
    <content><![CDATA[<h3 id="Target-Motivation"><a href="#Target-Motivation" class="headerlink" title="Target/Motivation"></a>Target/Motivation</h3><p>Traditional convolution-based generative adversarial networks synthesize images based on hierarchical local operations, where long-range dependency relation is implicitly modeled with a Markov chain.</p>
<p>传统的条件生成模型没有显示地去建立long-range dependency，难以捕获复杂的结构，the relation between distant locations relies on the Markovian modeling between convolutional layers.而是隐式地利用Markov chain来捕获long-range dependency。</p>
<p>It is still not sufficient for categories with complicated structures.这对于一些复杂结构的类别是不足够的。</p>
<p>而仅仅通过Markov chain，以stacked convs的形式来捕获long-range dependency是不够的。因为即使stacked convs具有large receptive field，但是仍然缺少model high-order relationship in distant locations的能力。</p>
<p>而这种high-order relationship的能力，是十分重要的。It presents the semantic correspondence that human perception is familiar with and sensitive about, e.g. symmetry of natural objects and correspondence among limbs.</p>
<p>为了有效地捕获long-range dependency，这里采用的是attentive normalization的方式。同时，这里是在建立语义一致的long-range feature之间的dependency，而不需要去考虑建立全局pixels之间的相关性。</p>
<a id="more"></a>

<p>相比于这篇文章提出的attentive normalization，SA-GAN（self-attention GAN）是直接以non-local/self-attention的方式来建立这种long-range dependency。然而self-attention需要去计算全局的相关性，也就是需要去建立任意两个pixel之间的相关性。这带来了很大的计算负担，尤其是当resolution变大时。</p>
<p>从只关注语义一致的信息的角度出发，这种方式去建立long-range dependency可以不需要考虑全局的信息并建立全局的关系，从而减少了计算资源的负担。</p>
<p>Attentive Normalization的工作是基于Instance Normalization的。后者仅仅考虑了spatial上的normalization，也就是channel-wise地进行normalization，而忽略了不同region可能对应不同的语义，从而应当对应不同的var和mean。</p>
<h3 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h3><p>现有方法难以有效地捕获long-range dependency。即使stacked convs具有大的感受野，但是仍然缺少model high-order relationship in distant locations的能力。</p>
<p>而这种high-order relationship的能力，是十分重要的。It presents the semantic correspondence that human perception is familiar with and sensitive about, e.g. symmetry of natural objects and correspondence among limbs.</p>
<p>From the computation perspective, pair-wise relationship calculation in the feature map demands quadratic complexity (regarding both time and space), limiting its application to large feature maps.但是对于现有的attention mechanism而言，建立任意两个feature pixels之间的联系非常消耗计算资源。</p>
<blockquote>
<p>这也是一个值得深入的方向；如果才能从对应同一个semantic regions中借来相应的特征呢？</p>
</blockquote>
<h3 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h3><p>In image generation, distant relationship modeling via attention mechanisms is proved to be effective for learning high-dimensional and complex image distribution.</p>
<p>对于学习高维特征和复杂的image distribution，attention mechanism有非常重要的意义。</p>
<p>It makes the input features approach independent and identical distribution by a shared mean and variance.Normalization通过共享mean和var来实现输入features的分布的统一。This property accelerates training convergence of neural networks and makes training deep networks feasible. Generally, after normalizing the given feature maps, these features are further affine-transformed, which is learned upon other features or conditions. These ways of conditional normalization can benefit the generator in creating more plausible label-relevant content.在normalization之后，还会继续做一个仿射变换，这个仿射变换是根据feature或者是condition学习得到的，从而使得生成的feature是label-relevant的。</p>
<h3 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h3><p>对于Attentive Normalization，首先是将feature map根据不同的semantics划分成不同的区域，并根据区域进行独立的normalization和de-normalization。所以分成两个模块，一个是semantic layout learning (SLL) module，另一个是根据semantic layout划分出来的regions来进行独立的regional normalization。</p>
<p><strong>Semantic layout learning module</strong></p>
<p>由semantics learning branch和self-sampling branch组成。对于semantics learning branch，是根据一定数量的convs来捕获对应具有不同的语义信息的regions。对于具有不同语义信息的regions，使用的是特定的filter来进行激活。这里基于这样的假设：每一个filter都对应一个特定semantic的entity；而对于self-sampling branch是用于辅助前面semantic learning branch的生成的。它对学习的semantic entity提供了一个正则约束，可以避免学习到无用的，与input feature无关的semantics。之后对于两个branch进行concat，并送入softmax中。</p>
<p><strong>semantics learning branch。</strong>在此之前，先了解一个assumption：For each feature point from the feature map of the image, it is determined by at least one entity. 也就是说，对于每一个neuron pixel，都是由至少一个entity决定的。These entities can be employed to know novel objects in different contexts to obtain an expressive representation.这些entity可以帮助在不同的context中提供帮助，获取相应的object的信息，从而获取更好的representation。这个假设中的每个pixel有至少一个对应的entity是最有帮助的，表明了一个feature map中，是可以根据相应level下的不同的entity对feature map进行划分的。对于semantic layout的生成，实际上就是在完成这个针对一个feature map中不同entity的划分的任务。这种方式可以提高一个group内特征的类内相似性。</p>
<p>首先设定有n个entity，并且定义features之间的相关性为dot-product的结果。semantic layout的生成过程是通过bp完成的。同时，通过不同entities的激活响应，将input feature map中的pixels融入不同的regions中。同时，为了让这些entities能够具有不同的pattern，还额外引入了正则项，这个正则项是针对<strong>learned weights</strong>即conv的参数进行的。</p>
<p>在实现过程中，采用的是一个卷积层，其中有n个filters。这个conv layer将输入feature map转换到$\R^{C\times H\times W}$子空间中。</p>
<p>然而，仅仅依靠这个分支，会使得模型将所有的feature pixels归到同一个class中，也就是生成的semantic layout为一个$1$.It is caused by not setting protocols to ban useless semantic entities that have low or no correlation with the input feature points.对于这个现象，作者认为是因为<strong>没有通过设置protocols来有效地避免关注到与输入feature points在语义上无关的或者说相关性很低的entities。</strong>因此提出了另一个branch，self-sampling branch来生成合理的semantic layout，避免trivial solution的出现。</p>
<p><strong>self-sampling branch。</strong>这个branch是用于给第一个semantic learning branch在从头学习得到semantic layout时提供正则约束。这个思路是受feature quantization的启发，reassigns empty clusters with the centroid of a non-empty cluster.通过对空簇的质心分配非空簇的质心来实现。</p>
<p>首先，采用self-attention的方式，从输入feature map得到k，q。对于q，这里采用sampling n个pixels的方式，认为采样的n个pixels，有一定几率得到对于不同的entity的pixels。这里采用dot-product的方式，通过计算这n个pixels和q中所有feature pixels之间的相似度，来完成类似聚类的操作，通过这种方式划分成n个不同的regions。实际上从attention的角度出发能够很好地理解这个branch。在self-sampling中，semantic layout就是attention scores。因为以self-attention的角度出发，对于每个q，关注的是k中那些特征和他相关；所以sampling的方式得到q，实际上就是相当于从原始任意两个pixels建立long-range dependencies变成建立指定n个pixels和global features之间的相关性，从而完成类似聚类的工作，得到一个semantic layout，通过<strong>Soft Semantic Layout Computation</strong>的方式来约束自适应学习的semantic layout，由此避免trivial solution的出现。对于Soft Semantic Layout Computation，通过将之前self-sampling branch得到的结果送入$1\times 1$卷积，并与semantic learning branch的结果进行相加，最后送入scale softmax得到soft semantic layout。</p>
<p><strong>Regional Normalization</strong></p>
<p>With the soft semantic layout, long-range relationship in feature maps is modeled by regional instance normalization.通过normalization，建立了同一个semantic下的long-range dependency。通过在semantic一致的区域内部共享mean和var，有助于提升模型的效果。通过和semantic layout点乘的方式，划分出不同的semantic相关的regions。并且针对每个region完成normalization。最后以residual的形式将进行了regional normalization的feature和原始feature进行融合。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Specifically, the input feature map is softly divided into several regions based on its internal semantic similarity, which are respectively normalized. It enhances consistency between distant regions with semantic correspondence.这里是通过normalization的方式来建立相同语义的features之间的long-range dependency。</p>
<p>In this paper, the authors normalize the input feature maps spatially according to the semantic layouts predicted from them.通过normalization的方式，在自动生成的semantic layout中进行。他有效地建立了long-range dependency，同时保留了空间维度上的semantics。</p>
<p>关于semantic layout的生成，基于以下两个观察：（1）a feature map can be viewed as a composition of multiple semantic entities。一个feature map可以视为由不同语义的实体所组成；（2）the deep layers in a neural network capture high-level semantics of the input images。深层的网络可以捕获输入image中的高级语义信息。</p>
<p>对于Attentive Normalization，能够以normalization的方式，通过使分布变得紧凑而增强semantics上相近的features之间的关系，而这个过程是忽略spatial limitation的。</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Fusion Network for Image Completion</title>
    <url>/2020/08/16/DeepFusionNet/</url>
    <content><![CDATA[<h3 id="Target-Motivation"><a href="#Target-Motivation" class="headerlink" title="Target/Motivation"></a>Target/Motivation</h3><p>Existing inpainting methods usually fail to harmonically blend the restored image into existing content, especially in the boundary area.</p>
<p>目标是解决hole内生成结果和surrounding相一致/和谐的问题。</p>
<p>The filled region must be perceptually plausible in the transition zone with sufficiently similar texture and consistent structure.</p>
<p> 作者认为，实现生成在边界处的平滑过渡在一些场景下比获得更加realistic的结果更为重要。因此这篇paper的重点集中在hole regions和known regions之间的过渡上。</p>
<p>之前的工作中，有针对边缘区域的pixel gradient进行优化的方法。对于融合质量的评价，通过判断两个图像梯度变化的一致性来判断。由此引申到restored content和known region之间连续性的保持上。</p>
<p>Patrick et al. proposed a method to iteratively optimize the pixel gradient in edge transitional region. Given two images, the fusion quality depends on the consistency of the gradient changes of these two images, which is similar with the relationship between the restored content and the known region in image completion.</p>
<a id="more"></a>

<h3 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h3><p>分为两派：（1）Contextual Attention派：认为应该从background中借来信息以填充hole regions；（2）EdgeConnect派：认为structure和texture更为重要，或者说认为structure consistency更为重要。这些方法中，例如edge之类的信息通常被用来保证structure的一致性。</p>
<h3 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h3><p>Existing inpainting methods usually fail to harmonically blend the restored image into existing content, especially in the boundary area.</p>
<p>Existing training strategies has problems. （1）the mission of image completion is to complete the unknown region only. It is actually hard to complete missing hole while keeping a strict identity mapping for known area. 对于image inpainting任务而言，重要的是reason、predict出hole regions中的content，但是如果同时需要对于known regions中的content完全恢复，即完成严格的恒等变换，是很困难的任务。（2）the inconsistent use of $I_{comp}$ and $I_{out}$ during training and testing, along with the rigid composition method, usually produces visible artifacts around the boundary of missing area.现有的一些方法中，在known regions部分，使用的是gt image，仅用于测试，而在训练过程中仅仅是利用reconstructed部分。因此这种训练、测试方式的不统一会导致生成结果在known regions和hole regions的交界处的不一致，从而产生artifacts。</p>
<p>从第一点和第二点中可以看到问题所在：如果在训练中使用gt的known regions，那么就会存在第一点的问题：很难同时学到恒等变换和predict的能力；如果在训练过程中，使用的是reconstructed的known regions，那么就会存在第二点问题：因为在测试的时候需要使用gt的known-region content，就会在边界的地方产生不和谐，artifacts。</p>
<p>这篇文章中从第二点出发，即train和test的时候，known regions中使用的是reconstructed contents，从而避免网络去学习不必要的恒等变换，同时去解决这种训练方式下，可能在测试过程中带来的boundary中存在artifacts的问题。</p>
<blockquote>
<p>这篇文章应该是从第二点出发，即train和test的时候，known regions中使用的是reconstructed contents。但是从第一点出发，是不是有一个pretrained的AutoEncoder会好很多？之前有一篇文章。</p>
</blockquote>
<h3 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h3><p>架构上，采用的UNet架构，同时对于decider的每一层，都使用一个fusion block。对于每一个layer中的fusion block，输入为原始corrupted图像$I_{in}$以及对应decoder输出的feature map $F$。以第$k$层为例，decoder的feature map为$F_k$，且corruted input image被resize到对应的分辨率，为$I_k$。对于$F_k$，将其通过变化$M$变换到RGB image，为$C_k$，并生成alpha map，来替代原先方法中的mask，实现smooth的known regions和hole regions的过渡。即对于第$k$层的输出image，为$I^{comp}_{k}=\alpha_k\odot M(F_k)+(1-\alpha_k) \odot I_k)=\alpha_k\odot C_k+(1-\alpha_k) \odot I_k)$。其中，$\alpha_k=A(I_k,F_k)$。</p>
<p>对于$A(\cdot)$采用的是stacked convs with kernel size =1, 3, 1，并且使用batch normalization和leakyrelu。 </p>
<p>The fusion block enables network to avoid learning unnecessary identity mapping while completing unknown region with soft transition near the boundary.</p>
<p>避免由于学习known regions处的features导致学习identity mapping，从而导致reason/predict能力减弱。并且对于boundary处的过渡更加平滑。</p>
<p>在训练过程中，使用的是$I^{comp}_k$，用于计算每一层的reconstruction loss；在测试时，只使用最后一层的$I_0$。所以这里在测试和训练都不涉及使用gt image的known-region部分，从而避免了需要model同时具有恒等变换的能力。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>提出了一个learnble fusion block，在pixel level完成known regions和hole regions之间的一致性的保持。并且生成一个alpha map，和mask类似，但是比mask有着更smooth的weights，尤其是在boundary处。这个alpha map在gradient descent optimization的过程中，不断地调整gt和restored image之间的balance，从而使得交界处更加smooth。也就是说，实际上这里不再是$(1-mask)gt_image + mask\cdot restored_image$的形式，而是通过一个alpha map来取代这里合成最终结果的mask的作用。这里的alpha map在边界处更加平滑，从而避免了过渡区域的不合理和突变。</p>
<p>核心的模块是fusion block。不仅完成关于known-region和hole-region features平滑的融合，同时还使用attention机制，将known regions的features转移到hole regions中；并且这个fusion block被嵌入在网络decoder的每一层。除此之外，充分利用不同layer的feature的性质，对不同layer使用structure loss和texture loss。</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Inception Generative Network for Cognitive Image Inpainting</title>
    <url>/2020/03/28/Deep-Inception-Generative-Network-for-Cognitive-Image-Inpainting/</url>
    <content><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>“existing learning-based methods often create artifacts and fallacious textures because of <strong>insufficient cognition understanding.</strong>“</p>
<p>“Previous generative networks are <strong>limited with single receptive type</strong> and <strong>give up pooling</strong> in consideration of <strong>detail sharpness.</strong>“</p>
<blockquote>
<p>之前的工作都局限于一种receptive field，同时为了sharpness（可以从resolution的角度来理解）放弃了一些pooling</p>
</blockquote>
<p>“Human cognition is <strong>constant</strong> regardless of the target attribute. <strong>As multiple receptive fields improve the ability of abstract image characterization</strong> and <strong>pooling can keep feature invariant</strong>, specifically, <strong>deep inception learning</strong> is adopted to promote <strong>high-level feature representation</strong> and <strong>enhance model learning capacity</strong> for local patches.”</p>
<a id="more"></a>

<blockquote>
<p>1）deep inception可以提升high-level feature的表达能力以及提升模型的学习能力</p>
<p>2）multi-scale receptive fields可以提升image特征的抽象能力</p>
</blockquote>
<p>==<strong>multi-scale的motivation</strong>==</p>
<p>“First, these methods often create <strong>boundary artifacts, distorted structures and blurry textures inconsistent with surrounding areas</strong> because of <strong>insufficient cognitive understanding</strong> and <strong>ineffectiveness</strong> of convolutional neural networks <strong>in modeling long-term correlations</strong> between contextual information and the hole regions”</p>
<blockquote>
<p>没有很好的感知context，同时也没有很好的建立contextual information和hole regions之间的long-term correlations</p>
</blockquote>
<p>“The filter used by traditional CNNs is a <strong>generalized linear model</strong>, Therefore, it is <strong>implicitly assumed that the features are linearly separable for extraction</strong>, but the actual case is often <strong>difficult to be linearly separable.</strong>“</p>
<blockquote>
<p>传统CNN使用的的filters都是linear模型，因此都假设在features是线性可分的。但是这与实际情况不太相符合</p>
</blockquote>
<p>“Most generative networks give up pooling and are limited with 3 × 3 convolutional kernels. This is obviously <strong>not possible</strong> to fully utilize its <strong>learning ability</strong> and <strong>cognitive understanding</strong> due to using <strong>only a single type of receptive fields.</strong>“</p>
<blockquote>
<p>仅仅使用一种receptive fields会严重影响模型的learning ability以及cognitive understanding感知理解能力</p>
</blockquote>
<p>“Given a dog in an image, from our <strong>human vision cognition</strong>, it is always a dog no matter <strong>where the target is, big or small it is, and rotated or not it is</strong>. Vision cognition keeps <strong>invariable.</strong>“</p>
<blockquote>
<p>从人类的视觉感知层面来看，对于旋转，缩放以及空间位置的不同都有很好的鲁棒性。所以如果要让模型也具有这样的能力1）CNN能够很好的适应空间位置的变换；2）对于旋转，传统CNN不确定，参考STN；3）==对于缩放，receptive field能够很好的解决这个问题，但是multi-scale能够更好适应这样的问题==</p>
</blockquote>
<p>“<strong>Deep inception learning</strong> is adopted to utilize more complex structures to abstract the data within <strong>diverse receptive fields and explore enough cognitive understanding.</strong>“</p>
<blockquote>
<p>Deep inception learning就可以很好地成为实现multi-scale receptive field的一个工具。</p>
</blockquote>
<p>“A novel generative network architecture using <strong>inception modules</strong> is proposed to <strong>enhance the abstraction ability of feature.</strong>“</p>
<p>“Existing inpainting networks do not <strong>possess nonlinearity for a convolutional layer.</strong>“</p>
<blockquote>
<p>现存的inpainting networks并不具有非线性的Conv</p>
</blockquote>
<p>“A solution is <strong>stacking convolution filters</strong> to generate higher-level feature representations to deal with practical problems and <strong>deeper models improve the abstract ability.</strong> Generally, the deeper the network is, the stronger the nonlinearity is.  In general, the most direct way to improve network performance is <strong>increasing the depth and width of the network.</strong>“</p>
<p>但是让网络变深变宽是要付出很多代价的：</p>
<p>“But this way has the following problems: it brings too <strong>many parameters</strong>, and if the training data set is limited, it is easy to produce <strong>over-fitting</strong>; The larger the network is, the greater the <strong>computational complexity</strong> is; deeper network is prone to leads to <strong>gradient diffusion</strong> and the model is <strong>difficult to optimize.</strong>“</p>
<blockquote>
<p>现有方法中使用堆叠的conv来提取high-level feature representations，并且用更深的网络来提高抽象能力，同时网络更深更宽就可以提升特征提取的能力。但是这样带来的代价太大了。</p>
<p>==Inception似乎也是为了解决deep问题提出的，所以inception对于提取更鲁棒的特征是有帮助的==</p>
</blockquote>
<p>==<strong>关于inception，实际上可以考虑将其应用到CAAG的two-branch结构中。</strong>==</p>
<p>“Special design can be done in the convolutional layer, so that the network can <strong>extract better features</strong> apart from stacking network convolutional layers. ==<strong>The idea of ’Network in Network’ (NIN) is introduced in <em>Network in network</em> firstly.</strong>==”</p>
<blockquote>
<p><strong>==NIN的做法和CAAG中的two-branch的做法很接近。具有参考意义==</strong></p>
</blockquote>
<p><strong>NIN</strong></p>
<p>“A micro neural network which is a <strong>potent function approximator inside a convolutional layer</strong> is instantiated. “</p>
<p><strong>Inception</strong></p>
<p>“Afterwards, inception modules are designed based on the framework of <em>Network in Network</em>“</p>
<p>“The main idea of inception is that an <strong>optimized local sparse structure</strong> in a convolutional neural network can <strong>be approximated and covered by a series of readily available dense substructures.</strong> Inception <strong>maintains the sparseness of the network</strong> and takes advantage of the high computational performance of dense matrices at the same time. It helps network <strong>to be deeper and wider without increasing the computation dramatically</strong>. <strong>Performance is improved significantly with the ability to extract more features under the same amount of computation.</strong>“</p>
<blockquote>
<p>Inception最厉害的地方在于可以在不显著增加计算量的情况下提取更多的特征</p>
</blockquote>
<hr>
<h3 id="所以这篇文章的本质还是从如何让模型具有更强大的特征提取能力，从而更好地知道要修复的内容是什么，达到更好的修复结果"><a href="#所以这篇文章的本质还是从如何让模型具有更强大的特征提取能力，从而更好地知道要修复的内容是什么，达到更好的修复结果" class="headerlink" title="所以这篇文章的本质还是从如何让模型具有更强大的特征提取能力，从而更好地知道要修复的内容是什么，达到更好的修复结果"></a>所以这篇文章的本质还是从如何让模型具有更强大的特征提取能力，从而更好地知道要修复的内容是什么，达到更好的修复结果</h3><hr>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><h4 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h4><p>“Inception is a micro network inside a layer and the NIN structure can <strong>utilize strong nonlinearity</strong> than vanilla convolutions <strong>in the same receptive field.</strong>“</p>
<p>“Generally, <strong>a small filter, a medium-sized filter, a large filter, and a pooling filter are consisted</strong>. It <strong>increases the width of the network</strong>. On the other hand, it also increases the adaptability of the network for <strong>multi-scale</strong> processing.”</p>
<blockquote>
<p>==Inception的方式蕴含着multi-scale==</p>
<p>==所以在CAAG中完全可以考虑NIN==</p>
</blockquote>
<p>Inception: “<strong>Large filter</strong> can <strong>cover larger regions</strong> of the receiving layers. <strong>Pooling</strong> is performed to <strong>reduce the size of the space and over-fitting.</strong> The <strong>topology</strong> of inception analyzes the <strong>relevant statistics of the upper layer</strong> and <strong>aggregates them into a highly related unit group</strong>. All the results are <strong>concatenated into a very deep feature map and the stitching means the fusion of different features.</strong>“</p>
<p><strong>同时这种Inception的方式也有利于节省参数</strong></p>
<p>“<strong>At the same time, more features can be extracted by superimposing more convolutions in the same receptive field.</strong>“</p>
<blockquote>
<p>inception中的一个特点或者说是multi-scale的一个特点就是对于image中的同一个对象，是通过多个不同size的filter对该对象所处的区域进行特征的提取，从而可以对一个对象提取出多个不同的特征，最后将它们融合。</p>
</blockquote>
<p><strong>==除了Inception，还可以考虑一下Res2Net来替代Inception。尤其是看到有使用inception的，就更觉得Res2Net可行了。==</strong></p>
<p>Inception versus Dilated Conv</p>
<p>“To satisfy <strong>spatial support</strong>, dilated convolution which allows to <strong>compute each output pixel</strong> with a <strong>much larger input area</strong> while still <strong>using the same amount of parameters and computational power</strong>. They believe that by using dilated convolutions at <strong>lower resolutions</strong>, the model can <strong>effectively see a larger area</strong> of the input image when computing each output pixel than with standard convolutional layers. A larger receptive field can be obtained by <strong>adopting inception learning.</strong>“</p>
<p>“And in this way, not only <strong>spatial support</strong> is satisfied, but also <strong>different views</strong> of images is obtained and <strong>different features</strong> are learned.”</p>
<blockquote>
<p>==要获取更大的感受野，可以通过应用inception来实现，也就是说，inception可以取代dilated conv==</p>
<p>==与此同时除了有更大的感受野，Inception相比于dilated conv还可以获取对一个物体不同尺度下的视角，以及提取出不同的特征==</p>
<p>==<strong>那么与此同时，实际上Res2Net也可以达到这样的效果，完全可以在CAAG中使用</strong>==</p>
</blockquote>
<p>“In order to avoid the expansion of parameters and calculations, large convolution kernels such as n × n filter are decomposed into n × 1 and 1 × n forms. This helps save parameters, speed up calculations, and avoid over-fitting.”</p>
<blockquote>
<p>Inception的一些tricks</p>
</blockquote>
<p>“Considering that <strong>pooling is embedded in Inception</strong>, this not only helps <strong>keep the ability of effective learning characteristics</strong>, but also <strong>avoids blurring the details and reducing the resolution caused by using pooling alone.</strong>“</p>
<blockquote>
<p>==与其说为什么要在inception中使用pooling，更不如说为什么要在inception这样的结构中加pooling：==</p>
<p>==pooling可以帮助提取更强大的feature，但是同样会造成resolution降低并带来blur。所以如果和inception中其他的filter相结合，就可以完美地克服这个问题==</p>
</blockquote>
<p>==<strong>所以如果要在CAAG中使用Res2Net，是可以考虑加上pooling</strong>==</p>
<p>“Apart from <strong>parallel combination</strong> of filters, there are also <strong>cascade forms</strong> for constructing inception. By cascading convolutions, more nonlinear features are acquired.”</p>
<blockquote>
<p>对于Inception的构建，除了parallel的形式，还有cascade的形式，也就是层叠的形式</p>
</blockquote>
<p>==对于Enc/Dec中还使用了不同的Inception Block==</p>
<p>“Such a network layer with an excellent <strong>local topology</strong> performing <strong>multiple convolution or pooling in parallel</strong> promotes learning of <strong>multiple features.</strong>“</p>
<p>“the intuition of effective ==<strong>multi-scale feature</strong>== representation helps bring <strong>enough cognitive understanding.</strong>“</p>
<p>“Inception improves <strong>cognitive logicality</strong>, make <strong>higher-quality inpainting results</strong> and <strong>ameliorate the of problem of artifacts</strong>, <strong>content discrepancy</strong>, <strong>color non-consistency and discrepancy</strong> which exist in previous works due to the <strong>lack of high level context representations and non-logic cognition.</strong>“</p>
<h4 id="Partial-Conv"><a href="#Partial-Conv" class="headerlink" title="Partial Conv"></a>Partial Conv</h4><p>“This would lead to color discrepancy, edge responses if these <strong>invalid pixels take part in convolution</strong>.”</p>
<h4 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h4><p>“The <strong>Encoder</strong> stage is to <strong>learn image features</strong> and it is <strong>a process of characterizing images</strong>. The <strong>Decoder</strong> stage is <strong>the process of restoring and decoding previously learned features to real images.</strong>“</p>
<blockquote>
<p>Encoder学习image的特征，并对应着刻画image特点的过程；Decoder对应恢复，对从Encoder中传入的feature进行decode得到real image的过程</p>
</blockquote>
<p>“This <strong>information</strong> generally consists of two categories: one is the <strong>overall environmental field information</strong>, and the other is the <strong>detailed information.</strong>“</p>
<p><strong>==对于filter的kernel size的选取会带来很多的不稳定因素：kernel size太大局部信息会丢失；kernel size太小难以获取全局信息，field information不够精确。==</strong></p>
<blockquote>
<p><strong>==这就是要使用Multi-scale的原因==</strong></p>
</blockquote>
<p>“Down-sampling is used to gradually <strong>reveal the environmental information</strong>, and the up-sampling process <strong>merges the learned features</strong> which <strong>include the environmental information</strong> during down-sampling to <strong>restore more details.</strong>“</p>
<p>网络的每一层都使用了BN，之后跟activation function</p>
<p>“A key point worth mentioning is that <strong>feature map size does not vary linearly layer by layer</strong> in the architecture. This is <strong>different</strong> from previous generative networks where feature map size changes with a factor of 2 between layers.”</p>
<blockquote>
<p>在这个模型中，feature map的size并不是线性递减的</p>
</blockquote>
<h3 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h3><p>1.reconstruction loss</p>
<p>只关注hole regions</p>
<blockquote>
<p>至于是否需要关注known regions。如果存在对于known region和hole region之间显式的限制的话，最好还是也对known regions使用reconstruction loss</p>
</blockquote>
<p>2.perceptual loss</p>
<p>3.style loss</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>EdgeConnect:Generative Image Inpainting With Adversarial Edge Learning</title>
    <url>/2020/03/28/EdgeConnect/</url>
    <content><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>“many existing image inpainting techniques generate over-smoothed and/or blurry regions, failing to reproduce fine details.”</p>
<p>“However, many techniques <strong>fail to reconstruct reasonable structures</strong> as they are commonly over-smoothed and/or blurry.”</p>
<p>“This paper develops a new approach for image inpainting that does a better job of <strong>reproducing filled regions exhibiting fine details.</strong>“</p>
<p>“The edge generator <strong>hallucinates edges</strong> of the missing region (both regular and irregular) of the image, and the image completion network <strong>fills in the missing regions</strong> using hallucinated edges <strong>as a priori.</strong>“</p>
<blockquote>
<p>同样是two-stage的架构，但是这里是先对反映structure信息的edge进行修复。之后再将这个edge作为先验来完成后续的修复工作</p>
</blockquote>
<p>“While these approaches are able to generate missing regions with <strong>meaningful structures</strong>, the generated regions are often <strong>blurry or suffer from artifacts</strong>, suggesting that these methods <strong>struggle to reconstruct high frequency information accurately.</strong>“</p>
<p>“Since image structure is well-represented in its edge mask, we show that it is possible to generate superior results by conditioning an image inpainting network on edges in the missing regions. Clearly, we do not have access to edges in the missing regions. Rather, we train an edge generator that hallucinates edges in these areas.”</p>
<p>“Our approach of “<strong>lines first, color next</strong>” is partly inspired by our understanding of how artists work.”</p>
<p>“<strong><em>In line drawing, the lines not only delineate and define spaces and shapes; they also play a vital role in the composition</em></strong>“</p>
<p>“Edge recovery, we suppose, <strong>is an easier task than image completion.</strong>“</p>
<p>“Our proposed model essentially <strong>decouples the recovery of high and low-frequency information</strong> of the inpainted region.”</p>
<a id="more"></a>

<blockquote>
<p>将任务按某种方式进行析构，这是一个很好的想法。</p>
</blockquote>
<hr>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>“Dilated convolutions with a <strong>dilation factor of two</strong> are used instead of regular convolutions in the residual layers, resulting in a receptive field of 205 at the final residual block. For discriminators, we use a 70×70 <strong>PatchGAN</strong> architecture, which <strong>determines whether or not overlapping image patches of size 70 × 70 are real</strong>. We use <strong>instance normalization across all layers</strong> of the network.”</p>
<blockquote>
<p>最关键的是在所有层中使用instance norm</p>
</blockquote>
<p>==”其实采用res2net的方式在encoder处使用是一个好的选择，例如在bottleneck处，encoder的two-branch输出后，并在CAttn和dilated conv之前使用”==</p>
<p>==CAAG的另一种motivation的说法：在decoder shallow处，这种由CA构建出来的long-distant correlation很微弱。==</p>
<h4 id="edge-generator"><a href="#edge-generator" class="headerlink" title="edge generator"></a>edge generator</h4><p>主要就是完成对edge的修复工作。</p>
<p>其实这里的关键点在于这个edge修复工作的loss应该如何构建</p>
<p>“We use $C_{gt}$ and $C_{pred}$ conditioned on $I_{gray}$ as inputs of the <strong>discriminator that predicts whether or not an edge map is real.</strong>“</p>
<p>“The network is trained with an objective comprised of an <strong>adversarial loss</strong> and <strong>feature-matching loss.</strong>“</p>
<p>“Spectral normalization (SN) further <strong>stabilizes training</strong> by <strong>scaling down weight matrices</strong> by their respective <strong>largest singular values</strong>, effectively <strong>restricting the Lip-schitz constant</strong> of the network to one.”</p>
<p>“Recent works suggest that generator can also benefit from SN by <strong>suppressing sudden changes of parameter and gradient values.</strong>“</p>
<blockquote>
<p>SN的作用</p>
</blockquote>
<p>“Although this was originally proposed to be used <strong>only on the discriminator</strong>, we <strong>apply SN to both generator and discriminator</strong>. Spectral normalization was chosen over <strong>Wasserstein GAN (WGAN)</strong>, as we found that <strong>WGAN was several times slower in our early tests.</strong>“</p>
<blockquote>
<p>在Generator和Discriminator上都应用SN，同时使用SN替代WGAN，因为在测试的时候发现WGAN在速度上慢了好几倍</p>
</blockquote>
<p>==<strong>可以考虑在CAAG上使用这个策略</strong>==</p>
<h4 id="image-conpletion-network"><a href="#image-conpletion-network" class="headerlink" title="image conpletion network"></a>image conpletion network</h4><p>对image进行修复，并将之前得到的edge map作为先验信息指导修复过程</p>
<p>“The composite edge map is constructed by <strong>combining the background region of ground truth edges with generated edges</strong> in the corrupted region from the previous stage.”</p>
<p>网络设计上没有什么太多特别的，主要是体现在objective functions上</p>
<p>“We noticed that the <strong>training time increases</strong> significantly if <strong>spectral normalization is included</strong>. We believe this is <strong>due to the network becoming too restrictive</strong> with the increased number of terms in the loss function.”</p>
<blockquote>
<p>作者认为之所以加入SN后训练时间会显著增加是因为使用了太多项loss function</p>
</blockquote>
<hr>
<h3 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h3><h4 id="edge-generator-1"><a href="#edge-generator-1" class="headerlink" title="edge generator"></a>edge generator</h4><p>1.feature loss</p>
<p>“The feature-matching loss $L_{FM}$ <strong>compares the activation maps in the intermediate layers of the discriminator</strong>. This <strong>stabilizes the training process</strong> by forcing the generator to produce results with <strong>representations that are similar to real images.</strong>“</p>
<p>“This is <strong>similar to perceptual loss</strong>, where activation maps are compared with those from the <strong>pre-trained VGG network.</strong> However, since the VGG network <strong>is not trained to produce edge information</strong>, it fails to capture the result that we seek in the initial stage.”</p>
<blockquote>
<p>之所以不使用perceptual loss而是使用他的翻版，是因为VGG-Net中没有需要的信息</p>
<p>==所以具体在perceptual loss这种对feature map进行度量的loss中应该使用什么样的网络可以根据需要考虑好。同时相比之下CSA的coherent loss就比较适合CAAG==</p>
</blockquote>
<h4 id="image-conpletion-network-1"><a href="#image-conpletion-network-1" class="headerlink" title="image conpletion network"></a>image conpletion network</h4><p><strong>1.l1 loss</strong></p>
<p>To ensure proper scaling, l1 loss is normalized by the mask size.</p>
<p><strong>2.adversarial loss</strong></p>
<p><strong>3.perceptual loss</strong></p>
<p>“perceptual loss <strong>penalizes</strong> results that are <strong>not perceptually similar to labels by defining a distance measure between activation maps of a pre-trained network.</strong>“</p>
<p><strong>4.style loss</strong></p>
<p>“style loss measures the differences between covariances of the activation maps.”</p>
<p>“style loss is an <strong>effective</strong> tool to combat <strong>‘checkerboard’</strong> artifacts caused by <strong>transpose convolution layers.</strong>“</p>
<blockquote>
<p>style loss可以解决由于转置卷积带来的”棋盘现象“</p>
</blockquote>
<p>==这一点实际上在CAAG的效果中有点明显，可以考虑使用==</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Coarse-to-Fine Image Inpainting via Region-Wise Convolutions and Non-Local Correlation</title>
    <url>/2020/03/27/Coarse-to-Fine/</url>
    <content><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>“Existing methods usually adopted the standard convolutional architecture over the corrupted image, where <strong>the same convolution filters</strong> try to <strong>restore the diverse information</strong> on both existing and missing regions, and meanwhile <strong>ignore the long-distance correlation</strong> among the regions.”</p>
<p>“<strong>Only relying on the surrounding areas</strong> inevitably leads to <strong>meaningless contents and artifacts</strong>, such as color discrepancy and blur.”</p>
<blockquote>
<p>莫不是和CSA一样的出发点：不能够仅仅关注于surrounding areas</p>
</blockquote>
<p>“We first propose <strong>region-wise convolutions</strong> to <strong>locally</strong> deal with the different types of regions, which can help <strong>exactly reconstruct existing regions and roughly infer the missing ones</strong> from existing regions at the same time.”</p>
<p>“Then, a <strong>non-local operation</strong> is introduced to <strong>globally</strong> model the correlation among different regions, promising <strong>visual consistency</strong> between missing and existing regions.”</p>
<blockquote>
<p>采用的是先Local后Global的策略。Local是为了预测和重建，而Global是为了保持一致性</p>
</blockquote>
<p>局部=&gt;关注细节，但是缺乏对整体的把握，这也是为什么会在hole regions和known regions之间存在边界处不连续的原因=&gt;关注的太局部</p>
<p>全局=&gt;对整体语义的把握更有优势，但是缺乏了对细节的掌握</p>
<a id="more"></a>

<h3 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h3><p>step1:</p>
<p>从已知区域中推测出丢失区域的信息=&gt;local</p>
<p>step2:</p>
<p>non-local operation=&gt;保证了整体的一致性，视觉上的连续性，丢失区域和已知区域边界处的连续性</p>
<p>step3:</p>
<p>（region-wise convolution）Local+（non-local operation）Global=&gt;Coarse-to-Fine</p>
<p>“In this paper, to generate desirable contents for missing regions, we treat the <strong>different types of regions using different convolution filters.</strong> <strong>Existing regions</strong> contain sufficient information and thus can be reconstructed based on themselves, while the <strong>missing ones</strong> without any information have to be <strong>inferred from the existing regions.</strong>“</p>
<p>“Therefore, we develop <strong>region-wise convolution operations</strong>, i.e., self-reconstruction and restoring from the existing regions, to <strong>separately deal with existing and missing regions.</strong>“</p>
<p>“The <strong>region-wise convolutions</strong> help <strong>infer the missing semantic contents</strong>, but inevitably cause the <strong>inconsistent appearance</strong> due to the <strong>ignorance of the correlation between existing and missing regions.</strong> We further propose a <strong>non-local operation to model the correlation among regions</strong>, thus generate more meaningful contents to connect them naturally.”</p>
<blockquote>
<p>使用region-wise conv来根据existing regions推测生成missing regions的结果，但是仅仅依赖于region-wise conv只能关注到local信息，会忽略全局的一致性以及known regions和missing regions之间的相关性，所以为了解决这个问题，使用non-local来建立known regions和missing regions之间的相关性。</p>
</blockquote>
<p>==<strong>还是原来的想法，可以考虑如何从CoarseNet入手，以减轻RefineNet的负担</strong>==</p>
<p><strong>==还有就是关于two-branch，虽然两个branch做的事情是不一样的，但是在输入以及branch的设计上并没有太多显式的引导，比如region-wise conv这样根据known regions和missing regions来使用不同的Conv以完成不同的任务==</strong></p>
<hr>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><h4 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a><strong>Encoder-Decoder</strong></h4><p>“To accomplish this goal, network E1, E2 serve as encoders in two stages respectively to <strong>extract semantic features</strong> from corresponding input images. “</p>
<p>“A decoder G composing of the proposed <strong>region-wised convolutional layers</strong> is employed after encoder E1 to <strong>restore the semantic contents for different regions</strong>, and <strong>generates the predicted image</strong> at the coarse stage.”</p>
<blockquote>
<p>实际上这个region-wised conv的目的也就很明了了：一个conv对于known regions，用于存储已知信息；而另一个conv对应missing regions，用于不根据contextual information来生成预测的结果。因此这种方式会需要CoarseNet和RefineNet，CoarseNet用于生成texture information，而RefineNet用于两个区域feature map之间相关性的保持。同时如果完全不依赖contextual information，这样生成的内容会严重缺乏与周围的一致性；同时还可以对两个conv生成的feature map进行一定的联动和结合，比如用一个loss来保持预测出来的feature的realistic。</p>
<p>所以这个region-wise的思路其实是很好的，只不过应该要有更聪明的处理方式。</p>
<p>这种region-wise的思想可以用在two-branch中，而且二者的motivation是一致的，都是为了针对region的不同来完成不一样的工作。</p>
</blockquote>
<h4 id="Inferring-Region-wise-Contents"><a href="#Inferring-Region-wise-Contents" class="headerlink" title="Inferring Region-wise Contents"></a>Inferring Region-wise Contents</h4><blockquote>
<p><strong>CAAG的新思路</strong></p>
<p>关于这部分，其实和two-branch的motivation很相似，而且在处理上也很相似，都是想把hallucination和attend操作分开来。实际上完全可以把Region-wise conv中的mask替换成gate。</p>
<p><strong>==关于Multi-scale部分，其实可以考虑Res2Net的方式==</strong></p>
<p>可以好好考虑一下这个Res2Net Block应该加在哪里，比如decoder的two-branch中？</p>
</blockquote>
<p>“Only relying on the same convolution filters, we can <strong>hardly restore the semantic features over different regions</strong>, which in practice usually <strong>leads to the visual artifacts</strong> such as color discrepancy, blur and obvious edge responses surrounding the missing regions.”</p>
<p>“Motivated by this observation, we first propose region-wise convolutions in the decoder network G at the coarse stage, and thus the decoder can <strong>separately generate the corresponding contents for different regions using different convolution filters.</strong>“</p>
<blockquote>
<p>根据不同的region提供不同的支持</p>
</blockquote>
<p>“In practice, we can accomplish region-wise convolutions by proportionally resizing the mask as feature maps down-sampled through the convolution layers. In this way, we can ensure that different regions <strong>can be easily distinguished</strong> according to the resized mask <strong>by channels</strong>, and thus the information in different regions can be transmitted consistently across layers.”</p>
<blockquote>
<p>在channel上划分出两个region对应的feature。实际上就是concat操作…</p>
</blockquote>
<h4 id="Modelling-Non-local-Correlation"><a href="#Modelling-Non-local-Correlation" class="headerlink" title="Modelling Non-local Correlation"></a><strong>Modelling Non-local Correlation</strong></h4><p>就是self-attention…</p>
<h3 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h3><p><strong>1.reconstruction loss</strong></p>
<p>将existing regions和missing regions区分开，分别给予考虑</p>
<p><strong>==2.correlation loss==</strong></p>
<p><strong>对correlation进行考虑，也就是说，对于attention map进行考虑，用于指导应该从哪里借特征</strong></p>
<p>“The reconstruction loss treats all pixels independently without consideration of their correlation, while in our observation the <strong>relationship among distant local patches</strong> plays a critical role in <strong>keeping the semantic and visual consistency</strong> between the generated missing regions and the existing ones. Therefore, we further introduce a <strong>correlation loss</strong> that can help to <strong>determine the expected non-local operation.</strong>“</p>
<p>“The correlation loss forces the model to generate images with <strong>semantic details much more close to the realistic image.</strong>“</p>
<p>“Here, different from the prior work of PConv, we only consider the non-local correlation for the composited image.”</p>
<p><strong>3.style loss</strong></p>
<p>“Although non-local correlation loss is capable of <strong>capturing long distance dependencies, enhancing the restoration of details</strong>, it still <strong>fails to avoid visual artifacts in unstable generative models.</strong>“</p>
<p>“Therefore, we append a style loss to <strong>produce clean results</strong> and **further refine the images **perceptually as a whole at the second stage.”</p>
<p>“The style loss <strong>focuses on</strong> the <strong>relationship between different channels</strong> to transfer the style for the composited image at the second stage.”</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Contextual-Based Imgae Inpainting:Infer, Match, and Translate</title>
    <url>/2020/03/21/Contextual-based-Infer-Match-Translate/</url>
    <content><![CDATA[<p>出发点：</p>
<p>就像之前对GAN的理解那样，实际上是为了找到一个mapping能将corrupted images转换到gt images上，然而之前的一些工作是直接找一个mapping，作者认为这样难度太大，所以将这个mapping分成了两个部分，由此简化任务的难度。将一个image translation problem划分成了两个image translation problems</p>
]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode58:最后一个单词的长度</title>
    <url>/2020/03/12/Leetcode58/</url>
    <content><![CDATA[<h3 id="58-最后一个单词的长度"><a href="#58-最后一个单词的长度" class="headerlink" title="58. 最后一个单词的长度"></a><a href="https://leetcode-cn.com/problems/length-of-last-word/">58. 最后一个单词的长度</a></h3><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>给你一个字符串 s，由若干单词组成，单词之间用空格隔开。返回字符串中最后一个单词的长度。如果不存在最后一个单词，请返回 0 。</p>
<p>单词 是指仅由字母组成、不包含任何空格字符的最大子字符串。</p>
<a id="more"></a>

<blockquote>
<p>示例 1：</p>
<p>输入：s = “Hello World”<br>输出：5</p>
<p>示例 2：</p>
<p>输入：s = “ “<br>输出：0</p>
</blockquote>
<p>提示：</p>
<blockquote>
<p>1 &lt;= s.length &lt;= 104<br>s 仅有英文字母和空格 ‘ ‘ 组成</p>
</blockquote>
<h4 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h4><p><strong>只需要考虑当前索引值对应的字符以及下一个索引值所对应的字符之间的关系即可。</strong></p>
<p><strong><em>\</em>-什么情况下需要开始计算最后一个单词的长度？**</strong></p>
<p>只要是当前索引值对应字符串中的字符不为’ ‘空格时，此时单词长度就应该加一；</p>
<p><strong><em>\</em>-什么情况下需要对最后一个单词长度清零？**</strong></p>
<p>只有在当前索引值对应的字符为’ ‘且下一个索引值对应的字符不为’ ‘且不为’\0’时才需要将累计的最后一个单词的长度清零。</p>
<p>因为当遭遇连续的空格和连续的非空格字符时是需要累计单词长度的。</p>
<p>同时考虑两个特殊情况：字符串长度分别为0和1，单独对二者进行处理即可。最终代码如下所示：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">lengthOfLastWord</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(s[<span class="number">0</span>]==<span class="string">&#x27;\0&#x27;</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> size = s.size(); <span class="comment">// not include &#x27;\0&#x27;</span></span><br><span class="line">        <span class="keyword">if</span>(size==<span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s[<span class="number">0</span>]==<span class="string">&#x27; &#x27;</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> num=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;size; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s[i]!=<span class="string">&#x27; &#x27;</span>) num++;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(s[i]==<span class="string">&#x27; &#x27;</span> &amp;&amp; s[i+<span class="number">1</span>]!=<span class="string">&#x27; &#x27;</span> &amp;&amp; s[i+<span class="number">1</span>]!=<span class="string">&#x27;\0&#x27;</span>) num=<span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> num;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode206反转链表</title>
    <url>/2020/03/12/Leetcode206/</url>
    <content><![CDATA[<h4 id="206-反转链表"><a href="#206-反转链表" class="headerlink" title="206. 反转链表"></a><a href="https://leetcode-cn.com/problems/reverse-linked-list/">206. 反转链表</a></h4><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>反转一个单链表。</p>
<a id="more"></a>

<p><strong>示例:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL</span><br><span class="line">输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL</span><br></pre></td></tr></table></figure>
<h4 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h4><p>很简单的思路：</p>
<p>只需要先找到链表的最后一个节点，并存储，作为反转后的链表的const head指针。整个过程只需要将原链表中的第一个节点插入原链表中最后一个节点之后即可。代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head==<span class="literal">NULL</span>) <span class="keyword">return</span> head;</span><br><span class="line">        ListNode* tail = findTail(head);</span><br><span class="line">        ListNode* cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur!=tail)&#123;</span><br><span class="line">            head = head-&gt;next;</span><br><span class="line">            insert_next(tail, cur);</span><br><span class="line">            cur=head;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tail;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">findTail</span><span class="params">(ListNode* head)</span></span>;</span><br><span class="line">    <span class="function">ListNode* <span class="title">insert_next</span><span class="params">(ListNode* head, ListNode* node)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">ListNode* <span class="title">Solution::findTail</span><span class="params">(ListNode* head)</span></span>&#123;</span><br><span class="line">        ListNode* cur=head;</span><br><span class="line">        <span class="keyword">if</span>(cur==<span class="literal">NULL</span>) <span class="keyword">return</span> head;</span><br><span class="line">        <span class="keyword">while</span>(cur-&gt;next!=<span class="literal">NULL</span>) cur=cur-&gt;next;</span><br><span class="line">        <span class="keyword">return</span> cur;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ListNode* <span class="title">Solution::insert_next</span><span class="params">(ListNode* head, ListNode* node)</span></span>&#123;</span><br><span class="line">    node-&gt;next=head-&gt;next;</span><br><span class="line">    head-&gt;next=node;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode13.罗马数字转整数</title>
    <url>/2020/03/05/Leetcode13/</url>
    <content><![CDATA[<h3 id="Leetcode-13-罗马数字转整数"><a href="#Leetcode-13-罗马数字转整数" class="headerlink" title="Leetcode 13. 罗马数字转整数"></a>Leetcode <a href="https://leetcode-cn.com/problems/roman-to-integer/">13. 罗马数字转整数</a></h3><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>罗马数字包含以下七种字符: I， V， X， L，C，D 和 M。</p>
<blockquote>
<p>字符          数值<br>I             1<br>V             5<br>X             10<br>L             50<br>C             100<br>D             500<br>M             1000<br>例如， 罗马数字 2 写做 II ，即为两个并列的 1。12 写做 XII ，即为 X + II 。 27 写做  XXVII, 即为 XX + V + II 。</p>
</blockquote>
<p>通常情况下，罗马数字中小的数字在大的数字的右边。但也存在特例，例如 4 不写做 IIII，而是 IV。数字 1 在数字 5 的左边，所表示的数等于大数 5 减小数 1 得到的数值 4 。同样地，数字 9 表示为 IX。这个特殊的规则只适用于以下六种情况：</p>
<p>I 可以放在 V (5) 和 X (10) 的左边，来表示 4 和 9。<br>X 可以放在 L (50) 和 C (100) 的左边，来表示 40 和 90。<br>C 可以放在 D (500) 和 M (1000) 的左边，来表示 400 和 900。<br>给定一个罗马数字，将其转换成整数。输入确保在 1 到 3999 的范围内。</p>
<a id="more"></a>

<h4 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h4><p>实际上只需要判断后一个字符是否比前一个字符对应的int值更大，并且是否满足题目中所举例的六种情况（后一个字符对应的int值是前一个字符对应的int值的5倍或者10倍），就可以确定出当前字符对应的数字在最终结果计算中对应的操作符号是加法还是减法。</p>
<p>举例说明string数据”IV”：</p>
<p>字符’I’对应的int值为1，字符’V’对应的int值为5，满足后一个字符对应的int值为前一个字符对应的int值的5倍或者10倍的关系，则字符’I’在最终结果计算中的操作符号是负号，即对应减法。</p>
<p>在代码实现中，利用两个数组分别按对应关系存放字符和数值，通过索引值建立二者的联系，并且对仅有一个字符的string进行特殊考虑。代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">char</span> charList[<span class="number">7</span>] = &#123;<span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;V&#x27;</span>, <span class="string">&#x27;X&#x27;</span>, <span class="string">&#x27;L&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;M&#x27;</span>&#125;;</span><br><span class="line">  <span class="keyword">int</span> intList[<span class="number">7</span>] = &#123;<span class="number">1</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">50</span>,<span class="number">100</span>,<span class="number">500</span>,<span class="number">1000</span>&#125;;</span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">romanToInt</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">     <span class="keyword">int</span> index=<span class="number">0</span>;</span><br><span class="line">     <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">     <span class="keyword">if</span>(s[<span class="number">1</span>]==<span class="string">&#x27;\0&#x27;</span>)&#123; <span class="keyword">return</span> intList[Locate(s[<span class="number">0</span>], charList)];&#125;</span><br><span class="line">     <span class="keyword">while</span>(s[index+<span class="number">1</span>]!=<span class="string">&#x27;\0&#x27;</span>)&#123;</span><br><span class="line">       <span class="keyword">if</span>(intList[Locate(s[index], charList)]*<span class="number">5</span>==intList[Locate(s[index+<span class="number">1</span>], charList)]</span><br><span class="line">       || intList[Locate(s[index], charList)]*<span class="number">10</span>==intList[Locate(s[index+<span class="number">1</span>], charList)])&#123;</span><br><span class="line">         res = res - intList[Locate(s[index], charList)]; </span><br><span class="line">       &#125;<span class="keyword">else</span> res = res + intList[Locate(s[index], charList)];</span><br><span class="line">       index++;</span><br><span class="line">     &#125;</span><br><span class="line">     res = res + intList[Locate(s[index], charList)];</span><br><span class="line">     <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">Locate</span><span class="params">(<span class="keyword">char</span> x, <span class="keyword">char</span>* c)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Solution::Locate</span><span class="params">(<span class="keyword">char</span> x, <span class="keyword">char</span>* c)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> index=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(c[index]==x) <span class="keyword">return</span> index;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(c[index]!=<span class="string">&#x27;\0&#x27;</span>)&#123;</span><br><span class="line">       index++;</span><br><span class="line">     &#125;<span class="keyword">else</span> <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
</search>
